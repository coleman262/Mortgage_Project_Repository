import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from sklearn.preprocessing import StandardScaler
from sqlalchemy import create_engine
import matplotlib.pyplot as plt


# Time2Vec class timestamps the data for the encoder
class Time2Vec(nn.Module):
    def __init__(self, kernel_size=10):
        super(Time2Vec, self).__init__()
        self.kernel_size = kernel_size
        self.wb = nn.Linear(1, 1)  # Linear component
        self.bb = nn.Parameter(torch.zeros(1))
        self.wa = nn.Linear(1, kernel_size - 1)  # Periodic component
        self.ba = nn.Parameter(torch.zeros(kernel_size - 1))

    def forward(self, x):
        linear = self.wb(x) + self.bb
        sin_trans = torch.sin(self.wa(x) + self.ba)
        return torch.cat([linear, sin_trans], -1)


# FinancialDataset determines the structure of the dataset for processing
class FinancialDataset(Dataset):
    def __init__(self, data, sequence_length=30, prediction_length=60, time2vec_dim=2):
        self.data = data
        self.sequence_length = sequence_length
        self.prediction_length = prediction_length
        self.time2vec = Time2Vec(time2vec_dim)

    def __len__(self):
        return len(self.data) - self.sequence_length - self.prediction_length

    def __getitem__(self, idx):
        time_feature = torch.Tensor(self.data.iloc[idx:idx + self.sequence_length][['index']].values.astype(np.float32))
        time_encoded = self.time2vec(time_feature / (24 * 60 * 60))
        additional_features = torch.Tensor(
            self.data.iloc[idx:idx + self.sequence_length][['t10y2y', 'usrec', 'dff']].values.astype(np.float32))
        X = torch.cat((time_encoded, additional_features), dim=1)
        y = torch.Tensor(self.data.iloc[idx + self.sequence_length:idx + self.sequence_length + self.prediction_length][['dgs10']].values.astype(np.float32))
        return X, y.squeeze(1)


class PositionalEncoding(nn.Module):
    def __init__(self, d_model, dropout=0.1, max_len=13403):
        super(PositionalEncoding, self).__init__()
        self.dropout = nn.Dropout(p=dropout)

        position = torch.arange(max_len).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2) * -(np.log(10000.0) / d_model))
        pe = torch.zeros(max_len, 1, d_model)
        pe[:, 0, 0::2] = torch.sin(position * div_term)
        pe[:, 0, 1::2] = torch.cos(position * div_term)
        self.register_buffer('pe', pe)

    def forward(self, x):
        x = x + self.pe[:x.size(0)]
        return self.dropout(x)

# Enhanced Transformer Class
class EnhancedTransformer(nn.Module):
    def __init__(self, input_dim, hidden_dim, prediction_length, time2vec_dim=10, num_layers=8, dropout=0.5):
        super(EnhancedTransformer, self).__init__()
        self.pos_encoder = PositionalEncoding(input_dim, dropout)
        transformer_layer = nn.TransformerEncoderLayer(d_model=input_dim, nhead=2, dim_feedforward=hidden_dim, dropout=dropout, batch_first=True)
        self.transformer_encoder = nn.TransformerEncoder(transformer_layer, num_layers=num_layers)
        self.fc = nn.Linear(input_dim, prediction_length)

    def forward(self, x):
        x = self.pos_encoder(x)
        out = self.transformer_encoder(x)
        out = self.fc(out[-1, :, :])  # Taking the output of the last sequence element
        return out

# Database connection parameters
db_host = 'localhost'
db_name = 'postgres'
db_user = 'postgres'
db_password = 'password123$'
table_name = 'model_data_daily'

# Database connection URL
db_url = f'postgresql+psycopg2://{db_user}:{db_password}@{db_host}/{db_name}'

# Create a SQLAlchemy engine
engine = create_engine(db_url)

# SQL query and data processing
try:
    # SQL query to include additional features
    sql_query = f"""
        SELECT "index", "dgs10", "t10y2y", "usrec", "dff"
        FROM {table_name}
        ORDER BY "index";
    """

    data = pd.read_sql_query(sql_query, engine)
    print(data.columns)

    if len(data) == 0:
        raise Exception("No data fetched from the database")

    # Preserve original dates for plotting
    data['date'] = pd.to_datetime(data['index'])
    # Convert 'index' from date to timestamp for model processing
    data['index'] = data['date'].astype(np.int64) // 10 ** 9
    data['index'] = (data['index'] - data['index'].min()) / (24 * 60 * 60)

    # Splitting the data into training and test sets
    split_ratio = 0.8
    split_idx = int(len(data) * split_ratio)
    train_data = data.iloc[:split_idx].copy()
    test_data = data.iloc[split_idx:].copy()

    # Scaling the data
    scaler_dgs10 = StandardScaler()
    scaler_features = StandardScaler()

    # Scale the 'dgs10' feature
    train_data['dgs10'] = scaler_dgs10.fit_transform(train_data[['dgs10']])
    test_data['dgs10'] = scaler_dgs10.transform(test_data[['dgs10']])

    # Scale the additional features
    train_data[['t10y2y', 'usrec', 'dff']] = scaler_features.fit_transform(train_data[['t10y2y', 'usrec', 'dff']])
    test_data[['t10y2y', 'usrec', 'dff']] = scaler_features.transform(test_data[['t10y2y', 'usrec', 'dff']])

    # Setting sequence length, prediction length, and time2vec dimension
    sequence_length = 30
    prediction_length = 60
    time2vec_dim = 7

    # Number of additional features
    num_additional_features = 3

    # Correct calculation of input dimension
    input_dim = time2vec_dim + num_additional_features

    # Define the hidden dimension for the RNN
    hidden_dim = 64

    # Creating datasets and dataloaders
    train_dataset = FinancialDataset(train_data, sequence_length, prediction_length, time2vec_dim)
    test_dataset = FinancialDataset(test_data, sequence_length, prediction_length, time2vec_dim)

    batch_size = 6
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

    # Model, loss function, and optimizer
    model = EnhancedTransformer(input_dim, hidden_dim, prediction_length, time2vec_dim)
    criterion = nn.MSELoss()
    optimizer = optim.Adam(model.parameters(), lr=0.001)


    # Training loop
    num_epochs = 50
    for epoch in range(num_epochs):
        model.train()
        total_loss = 0
        for batch_x, batch_y in train_loader:
            # Reshape batch_x to match Transformer input
            batch_x = batch_x.permute(1, 0, 2)  # Reshape to (seq_length, batch_size, features)
            optimizer.zero_grad()
            output = model(batch_x)
            loss = criterion(output, batch_y)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
        print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {total_loss / len(train_loader):.4f}')

    model.eval()
    with torch.no_grad():
        # Extract the last sequence of data from the entire dataset
        last_sequence_idx = len(data) - sequence_length
        last_sequence = data.iloc[last_sequence_idx:last_sequence_idx + sequence_length]

        # Prepare the sequence for the model
        time_feature = torch.Tensor(last_sequence[['index']].values.astype(np.float32))
        time_encoded = train_dataset.time2vec(time_feature)
        additional_features = torch.Tensor(last_sequence[['t10y2y', 'usrec', 'dff']].values.astype(np.float32))
        last_sequence_X = torch.cat((time_encoded, additional_features), dim=1)

        # Reshape to match batch size of 1
        last_sequence_X = last_sequence_X.unsqueeze(0)

        # Predicting forward
        output = model(last_sequence_X)
        original_predictions = scaler_dgs10.inverse_transform(output.numpy()).flatten()
        train_data['dgs10'] = scaler_dgs10.inverse_transform(train_data[['dgs10']])
        test_data['dgs10'] = scaler_dgs10.inverse_transform(test_data[['dgs10']])

        # Scaling back the additional features for train and test data
        train_data[['t10y2y', 'usrec', 'dff']] = scaler_features.inverse_transform(train_data[['t10y2y', 'usrec', 'dff']])
        test_data[['t10y2y', 'usrec', 'dff']] = scaler_features.inverse_transform(test_data[['t10y2y', 'usrec', 'dff']])



        # Generating future dates starting from the last date in the actual dataset
        last_date = pd.to_datetime(data['date'].iloc[-1])
        future_dates = [last_date + pd.Timedelta(days=i) for i in range(1, prediction_length + 1)]
        print(future_dates)

        # Creating DataFrame for the predictions
        original_predictions = original_predictions[:len(future_dates)]
        future_prediction_df = pd.DataFrame({'Date': future_dates, 'Predicted_dgs10': original_predictions})
        print("Future Predictions:")
        print(future_prediction_df)


        # Plotting

        plt.figure(figsize=(12, 6))
        plt.plot(data['date'], data['dgs10'], label='Actual Data', color='blue')  # Plotting original data

        plt.plot(test_data['date'], test_data['dgs10'], label='Test Data', color='purple')
        plt.plot(future_dates, original_predictions, label='Future Projections', color='green')

        plt.xlabel('Date')
        plt.ylabel('dgs10')
        plt.title('Model Predictions - 30 seq')
        plt.legend()
        plt.grid(True)
        plt.show()
except Exception as e:
    print("Error: Unable to fetch data from the database or preprocess data")
    print(e)
finally:
    engine.dispose()
